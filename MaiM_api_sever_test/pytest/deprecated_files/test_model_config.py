#!/usr/bin/env python3
"""
æµ‹è¯•æ¨¡å‹é…ç½®æ˜¯å¦æ­£ç¡®
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.llm_models.utils_model import LLMRequest
from src.config.config import model_config


async def test_planner_model():
    """æµ‹è¯•planneræ¨¡å‹é…ç½®"""
    print("ğŸ§ª æµ‹è¯•planneræ¨¡å‹é…ç½®...")

    try:
        # è·å–planneré…ç½®
        planner_config = model_config.model_task_config.planner
        print(f"ğŸ“‹ Planneræ¨¡å‹åˆ—è¡¨: {planner_config.model_list}")
        print(f"ğŸŒ¡ï¸ æ¸©åº¦: {planner_config.temperature}")
        print(f"ğŸ“ æœ€å¤§tokens: {planner_config.max_tokens}")

        # åˆ›å»ºLLMè¯·æ±‚
        llm_request = LLMRequest(planner_config, "planner_test")

        # æµ‹è¯•ç®€å•è¯·æ±‚
        test_prompt = 'ä½ å¥½ï¼Œè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚è¯·å›å¤ä¸€ä¸ªç®€å•çš„JSONï¼š{"action": "test", "reason": "æµ‹è¯•åŸå› "}'

        print("ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
        response, (reasoning, model_name, tool_calls) = await llm_request.generate_response_async(
            prompt=test_prompt, temperature=0.3, max_tokens=100
        )

        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_name}")
        print(f"ğŸ’¬ å“åº”å†…å®¹: {response}")
        print(f"ğŸ§  æ¨ç†å†…å®¹: {reasoning}")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_replyer_model():
    """æµ‹è¯•replyeræ¨¡å‹é…ç½®"""
    print("\nğŸ§ª æµ‹è¯•replyeræ¨¡å‹é…ç½®...")

    try:
        # è·å–replyeré…ç½®
        replyer_config = model_config.model_task_config.replyer
        print(f"ğŸ“‹ Replyeræ¨¡å‹åˆ—è¡¨: {replyer_config.model_list}")
        print(f"ğŸŒ¡ï¸ æ¸©åº¦: {replyer_config.temperature}")
        print(f"ğŸ“ æœ€å¤§tokens: {replyer_config.max_tokens}")

        # åˆ›å»ºLLMè¯·æ±‚
        llm_request = LLMRequest(replyer_config, "replyer_test")

        # æµ‹è¯•ç®€å•è¯·æ±‚
        test_prompt = "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ä¸€ä¸‹è¿™ä¸ªæµ‹è¯•æ¶ˆæ¯ã€‚"

        print("ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
        response, (reasoning, model_name, tool_calls) = await llm_request.generate_response_async(
            prompt=test_prompt, temperature=0.3, max_tokens=100
        )

        print(f"âœ… ä½¿ç”¨æ¨¡å‹: {model_name}")
        print(f"ğŸ’¬ å“åº”å†…å®¹: {response}")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ¨¡å‹é…ç½®æµ‹è¯•")
    print("=" * 50)

    # æµ‹è¯•planneræ¨¡å‹
    planner_success = await test_planner_model()

    # æµ‹è¯•replyeræ¨¡å‹
    replyer_success = await test_replyer_model()

    print("\n" + "=" * 50)
    print("ğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   Planneræ¨¡å‹: {'âœ… é€šè¿‡' if planner_success else 'âŒ å¤±è´¥'}")
    print(f"   Replyeræ¨¡å‹: {'âœ… é€šè¿‡' if replyer_success else 'âŒ å¤±è´¥'}")

    if planner_success and replyer_success:
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹é…ç½®æµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
