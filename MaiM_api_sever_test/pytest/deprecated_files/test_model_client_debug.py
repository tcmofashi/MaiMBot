#!/usr/bin/env python3
"""
è°ƒè¯•æ¨¡å‹å®¢æˆ·ç«¯é…ç½®é—®é¢˜
"""

import sys
import asyncio
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.config.config import model_config
from src.llm_models.model_client.base_client import client_registry
from src.llm_models.utils_model import LLMRequest
from src.common.logger import get_logger

logger = get_logger("model_client_debug")


async def test_model_client():
    """æµ‹è¯•æ¨¡å‹å®¢æˆ·ç«¯é…ç½®å’Œè°ƒç”¨"""

    print("=" * 60)
    print("ğŸ” æ¨¡å‹å®¢æˆ·ç«¯é…ç½®è°ƒè¯•")
    print("=" * 60)

    # 1. æ£€æŸ¥æ¨¡å‹é…ç½®
    print("\nğŸ“‹ æ£€æŸ¥æ¨¡å‹é…ç½®...")
    try:
        r1_model_info = model_config.get_model_info("r1")
        print(f"âœ… R1æ¨¡å‹ä¿¡æ¯: {r1_model_info}")
        print(f"   - æ¨¡å‹æ ‡è¯†ç¬¦: {r1_model_info.model_identifier}")
        print(f"   - APIæä¾›å•†: {r1_model_info.api_provider}")

        # æ£€æŸ¥APIæä¾›å•†é…ç½®
        provider = model_config.get_provider(r1_model_info.api_provider)
        print(f"âœ… APIæä¾›å•†ä¿¡æ¯: {provider}")
        print(f"   - åç§°: {provider.name}")
        print(f"   - Base URL: {provider.base_url}")
        print(f"   - å®¢æˆ·ç«¯ç±»å‹: {provider.client_type}")
        print(f"   - è¶…æ—¶: {provider.timeout}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹é…ç½®é”™è¯¯: {e}")
        return False

    # 2. æ£€æŸ¥å®¢æˆ·ç«¯æ³¨å†Œ
    print("\nğŸ”§ æ£€æŸ¥å®¢æˆ·ç«¯æ³¨å†Œ...")
    try:
        # è·å–å®¢æˆ·ç«¯
        client = client_registry.get_client_class_instance(provider)
        print(f"âœ… å®¢æˆ·ç«¯è·å–æˆåŠŸ: {type(client).__name__}")

        # æ£€æŸ¥å®¢æˆ·ç«¯é…ç½®
        if hasattr(client, "client"):
            openai_client = client.client
            print(f"   - Base URL: {openai_client.base_url}")
            print(f"   - API Key: {'***' + openai_client.api_key[-10:] if openai_client.api_key else 'None'}")
            print(f"   - Timeout: {openai_client.timeout}")

    except Exception as e:
        print(f"âŒ å®¢æˆ·ç«¯è·å–å¤±è´¥: {e}")
        return False

    # 3. æµ‹è¯•æ¨¡å‹è°ƒç”¨
    print("\nğŸš€ æµ‹è¯•æ¨¡å‹è°ƒç”¨...")
    try:
        # åˆ›å»ºLLMRequestå®ä¾‹
        llm_request = LLMRequest(model_set=model_config.model_task_config.planner, request_type="test")
        print("âœ… LLMRequeståˆ›å»ºæˆåŠŸ")
        print(f"   - æ¨¡å‹åˆ—è¡¨: {llm_request.model_for_task.model_list}")

        # æµ‹è¯•è°ƒç”¨
        print("ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
        response, (reasoning_content, model_name, tool_calls) = await llm_request.generate_response_async(
            prompt="ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ä¸€ä¸‹", max_tokens=50, temperature=0.3
        )

        print("âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸ!")
        print(f"   - å“åº”å†…å®¹: {response[:100] if response else 'None'}")
        print(f"   - æ¨ç†å†…å®¹: {reasoning_content[:100] if reasoning_content else 'None'}")
        print(f"   - ä½¿ç”¨æ¨¡å‹: {model_name}")

        return True

    except Exception as e:
        print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def test_direct_openai_client():
    """ç›´æ¥æµ‹è¯•OpenAIå®¢æˆ·ç«¯"""
    print("\nğŸ”§ ç›´æ¥æµ‹è¯•OpenAIå®¢æˆ·ç«¯...")

    try:
        from openai import AsyncOpenAI

        # è·å–é…ç½®
        r1_model_info = model_config.get_model_info("r1")
        provider = model_config.get_provider(r1_model_info.api_provider)

        # åˆ›å»ºå®¢æˆ·ç«¯
        client = AsyncOpenAI(
            base_url=provider.base_url, api_key=provider.api_key, timeout=provider.timeout, max_retries=0
        )

        print(f"ğŸ“¤ ç›´æ¥è°ƒç”¨API: {r1_model_info.model_identifier}")

        # æµ‹è¯•è°ƒç”¨
        response = await client.chat.completions.create(
            model=r1_model_info.model_identifier,
            messages=[{"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•å›å¤"}],
            max_tokens=50,
            temperature=0.3,
        )

        print("âœ… ç›´æ¥è°ƒç”¨æˆåŠŸ!")
        print(f"   - å“åº”: {response.choices[0].message.content}")

        return True

    except Exception as e:
        print(f"âŒ ç›´æ¥è°ƒç”¨å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å¼€å§‹æ¨¡å‹å®¢æˆ·ç«¯è°ƒè¯•...")

    # æµ‹è¯•ç³»ç»Ÿå†…éƒ¨è°ƒç”¨
    success1 = await test_model_client()

    # æµ‹è¯•ç›´æ¥è°ƒç”¨
    success2 = await test_direct_openai_client()

    print("\n" + "=" * 60)
    print("ğŸ“Š è°ƒè¯•ç»“æœæ€»ç»“")
    print("=" * 60)
    print(f"ç³»ç»Ÿå†…éƒ¨è°ƒç”¨: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
    print(f"ç›´æ¥APIè°ƒç”¨: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")

    if success2 and not success1:
        print("\nğŸ” åˆ†æ: ç›´æ¥APIè°ƒç”¨æˆåŠŸä½†ç³»ç»Ÿå†…éƒ¨è°ƒç”¨å¤±è´¥")
        print("   å¯èƒ½çš„åŸå› :")
        print("   1. å®¢æˆ·ç«¯é…ç½®é—®é¢˜")
        print("   2. æ¶ˆæ¯æ ¼å¼è½¬æ¢é—®é¢˜")
        print("   3. é”™è¯¯å¤„ç†æœºåˆ¶é—®é¢˜")
    elif not success1 and not success2:
        print("\nğŸ” åˆ†æ: ä¸¤ç§è°ƒç”¨éƒ½å¤±è´¥")
        print("   å¯èƒ½çš„åŸå› :")
        print("   1. APIé…ç½®é”™è¯¯")
        print("   2. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("   3. APIå¯†é’¥é—®é¢˜")
    elif success1 and success2:
        print("\nğŸ‰ ä¸¤ç§è°ƒç”¨éƒ½æˆåŠŸ!")

    return success1


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
